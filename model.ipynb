{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTANTS\n",
    "MAX_LENGTH = 40\n",
    "VOCABULARY_SIZE = 11821\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "EMBEDDING_DIM = 512\n",
    "UNITS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pickle.load(open('saved_models/vocab_coco_1676709587.file', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCABULARY_SIZE,\n",
    "    standardize=None,\n",
    "    output_sequence_length=MAX_LENGTH,\n",
    "    vocabulary=vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary(),\n",
    "    invert=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Encoder():\n",
    "    inception_v3 = tf.keras.applications.InceptionV3(\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    inception_v3.trainable = False\n",
    "\n",
    "    output = inception_v3.output\n",
    "    output = tf.keras.layers.Reshape(\n",
    "        (-1, output.shape[-1]))(output)\n",
    "\n",
    "    cnn_model = tf.keras.models.Model(inception_v3.input, output)\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embed_dim, activation=\"relu\")\n",
    "\n",
    "    def call(self, x, training):\n",
    "        x = self.layer_norm_1(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        attn_output = self.attention(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            attention_mask=None,\n",
    "            training=training\n",
    "        )\n",
    "\n",
    "        x = self.layer_norm_2(x + attn_output)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = tf.keras.layers.Embedding(\n",
    "            vocab_size, embed_dim)\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(\n",
    "            max_len, embed_dim, input_shape=(None, max_len))\n",
    "\n",
    "    def call(self, input_ids):\n",
    "        length = tf.shape(input_ids)[-1]\n",
    "        position_ids = tf.range(start=0, limit=length, delta=1)\n",
    "        position_ids = tf.expand_dims(position_ids, axis=0)\n",
    "\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        return token_embeddings + position_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, embed_dim, units, num_heads):\n",
    "        super().__init__()\n",
    "        self.embedding = Embeddings(\n",
    "            tokenizer.vocabulary_size(), embed_dim, MAX_LENGTH)\n",
    "\n",
    "        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "        self.ffn_layer_1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.ffn_layer_2 = tf.keras.layers.Dense(embed_dim)\n",
    "\n",
    "        self.out = tf.keras.layers.Dense(\n",
    "            tokenizer.vocabulary_size(), activation=\"softmax\")\n",
    "\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "    def call(self, input_ids, encoder_output, training, mask=None):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "\n",
    "        combined_mask = None\n",
    "        padding_mask = None\n",
    "\n",
    "        if mask is not None:\n",
    "            causal_mask = self.get_causal_attention_mask(embeddings)\n",
    "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
    "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
    "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
    "\n",
    "        attn_output_1 = self.attention_1(\n",
    "            query=embeddings,\n",
    "            value=embeddings,\n",
    "            key=embeddings,\n",
    "            attention_mask=combined_mask,\n",
    "            training=training\n",
    "        )\n",
    "\n",
    "        out_1 = self.layernorm_1(embeddings + attn_output_1)\n",
    "\n",
    "        attn_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_output,\n",
    "            key=encoder_output,\n",
    "            attention_mask=padding_mask,\n",
    "            training=training\n",
    "        )\n",
    "\n",
    "        out_2 = self.layernorm_2(out_1 + attn_output_2)\n",
    "\n",
    "        ffn_out = self.ffn_layer_1(out_2)\n",
    "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
    "        ffn_out = self.ffn_layer_2(ffn_out)\n",
    "\n",
    "        ffn_out = self.layernorm_3(ffn_out + out_2)\n",
    "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
    "        preds = self.out(ffn_out)\n",
    "        return preds\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, cnn_model, encoder, decoder, image_aug=None):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.image_aug = image_aug\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker = tf.keras.metrics.Mean(name=\"accuracy\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.image_aug(inputs[0])\n",
    "        x = self.cnn_model(x)\n",
    "        x = self.encoder(x, False)\n",
    "        x = self.decoder(inputs[2],x,training=inputs[1],mask=None)\n",
    "        return x\n",
    "#         x = self.image_aug(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred, mask):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
    "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
    "        accuracy = tf.math.logical_and(mask, accuracy)\n",
    "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "    \n",
    "\n",
    "    def compute_loss_and_acc(self, img_embed, captions, training=True):\n",
    "        encoder_output = self.encoder(img_embed, training=True)\n",
    "        y_input = captions[:, :-1]\n",
    "        y_true = captions[:, 1:]\n",
    "        mask = (y_true != 0)\n",
    "        y_pred = self.decoder(\n",
    "            y_input, encoder_output, training=True, mask=mask\n",
    "        )\n",
    "        loss = self.calculate_loss(y_true, y_pred, mask)\n",
    "        acc = self.calculate_accuracy(y_true, y_pred, mask)\n",
    "        return loss, acc\n",
    "\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        imgs, captions = batch\n",
    "\n",
    "        if self.image_aug:\n",
    "            imgs = self.image_aug(imgs)\n",
    "        \n",
    "        img_embed = self.cnn_model(imgs)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, acc = self.compute_loss_and_acc(\n",
    "                img_embed, captions\n",
    "            )\n",
    "    \n",
    "        train_vars = (\n",
    "            self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        )\n",
    "        grads = tape.gradient(loss, train_vars)\n",
    "        self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state(acc)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "    \n",
    "\n",
    "    def test_step(self, batch):\n",
    "        imgs, captions = batch\n",
    "\n",
    "        img_embed = self.cnn_model(imgs)\n",
    "\n",
    "        loss, acc = self.compute_loss_and_acc(\n",
    "            img_embed, captions, training=False\n",
    "        )\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state(acc)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.acc_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_path(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
    "    img = img / 255.\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(img, caption_model):\n",
    "    if isinstance(img, str):\n",
    "        img = load_image_from_path(img)\n",
    "\n",
    "    if isinstance(img, np.ndarray):\n",
    "        img = tf.convert_to_tensor(img)\n",
    "\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    img_embed = caption_model.cnn_model(img)\n",
    "    img_encoded = caption_model.encoder(img_embed, training=False)\n",
    "\n",
    "    y_inp = '[start]'\n",
    "    for i in range(MAX_LENGTH-1):\n",
    "        tokenized = tokenizer([y_inp])[:, :-1]\n",
    "        mask = tf.cast(tokenized != 0, tf.int32)\n",
    "        pred = caption_model.decoder(\n",
    "            tokenized, img_encoded, training=False, mask=mask)\n",
    "\n",
    "        pred_idx = np.argmax(pred[0, i, :])\n",
    "        pred_word = idx2word(pred_idx).numpy().decode('utf-8')\n",
    "        if pred_word == '[end]':\n",
    "            break\n",
    "\n",
    "        y_inp += ' ' + pred_word\n",
    "\n",
    "    y_inp = y_inp.replace('[start] ', '')\n",
    "    return y_inp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption_model():\n",
    "    encoder = TransformerEncoderLayer(EMBEDDING_DIM, 1)\n",
    "    decoder = TransformerDecoderLayer(EMBEDDING_DIM, UNITS, 8)\n",
    "\n",
    "    cnn_model = CNN_Encoder()\n",
    "\n",
    "    caption_model = ImageCaptioningModel(\n",
    "        cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=None,\n",
    "    )\n",
    "\n",
    "    def call_fn(batch, training):\n",
    "        return batch\n",
    "\n",
    "    caption_model.call = call_fn\n",
    "    sample_x, sample_y = tf.random.normal((1, 299, 299, 3)), tf.zeros((1, 40))\n",
    "\n",
    "    caption_model((sample_x, sample_y))\n",
    "\n",
    "    sample_img_embed = caption_model.cnn_model(sample_x)\n",
    "    sample_enc_out = caption_model.encoder(sample_img_embed, training=False)\n",
    "    caption_model.decoder(sample_y, sample_enc_out, training=False)\n",
    "\n",
    "    caption_model.load_weights('saved_models\\model.h5')\n",
    "    caption_model.summary()\n",
    "    caption_model.get_weights()\n",
    "\n",
    "    return caption_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "865d8b2eb28e274047ba64063dfb6a2aabf0dfec4905d304d7a76618dae6fdd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
